{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dec4b6",
   "metadata": {},
   "source": [
    "# Week 3 stats exercises\n",
    "\n",
    "## Problem 1.\n",
    "\n",
    "\n",
    "(Computer exercise) Let’s assume you have **N = 100** observations of a $X_i \\sim \\text{Bernoulli}(p)$ random variable with **k = 23 successes**  \n",
    "   (i.e. $X_i = 1$ in 23 out of the 100 repeats).\n",
    "\n",
    "###  1.1. Find the **MLE estimator** $\\hat{p}$ for $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b661035",
   "metadata": {},
   "source": [
    "PDF for Bernoulli distribution is $f(x; p)=p^x(1-p)^{1-x}$ for $x \\in \\{0,1\\}$\n",
    "\n",
    "The unknown paramater is $p$. Then\n",
    "\n",
    "$\\mathcal{L}_n(\\theta) = \\prod_{i=1}^n f(X_i; p)$\n",
    "\n",
    "$\\mathcal{L}_n(p) = p^{x_1}(1-p)^{1-x_1} \\cdot ... \\cdot p^{x_n}(1-p)^{1-x_n}$\n",
    "\n",
    "$\\mathcal{L}_n(p) = p^{x_1 + ... + x_n} \\cdot (1-p)^{n-(x_1 + ... + x_n)}$\n",
    "\n",
    "Where $x_1 + ... + x_n = S$\n",
    "\n",
    "$\\mathcal{L}_n(p) = p^{S} \\cdot (1-p)^{n-S}$\n",
    "\n",
    "Taking the log:\n",
    "\n",
    "$\\mathcal{l}_n(p) = \\log( p^{S} \\cdot (1-p)^{n-S})$\n",
    "\n",
    "$\\mathcal{l}_n(p) = S\\cdot \\log(p) + (n-S)\\log(1-p) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59519c3",
   "metadata": {},
   "source": [
    "Log derivate rule:\n",
    "\n",
    "$\\frac{d}{dx}\\log(x) = \\frac{1}{x}$\n",
    "\n",
    "\n",
    "Take derivate of $\\mathcal{l}_n(p)$\n",
    "\n",
    "$\\mathcal{l}_n(p) = S\\cdot \\log(p) + (n-S)\\log(1-p) $\n",
    "\n",
    "$\\frac{d}{dp} \\mathcal{l}_n(p) = S\\frac{1}{p} - (n-S)\\frac{1}{1-p} $\n",
    "\n",
    "$\\frac{d}{dp} \\mathcal{l}_n(p) =\\frac{S}{p} - \\frac{n-S}{1-p} $\n",
    "\n",
    "Set derivative to 0:\n",
    "\n",
    "$\\frac{S}{p} - \\frac{n-S}{1-p} = 0$\n",
    "\n",
    "$S(1-p) - p(n-S) = 0$\n",
    "\n",
    "$S - Sp + Sp  -np = 0$\n",
    "\n",
    "$S -np = 0$\n",
    "\n",
    "$S  = np$\n",
    "\n",
    "$p=\\frac{S}{n} $\n",
    "\n",
    "$p=\\frac{23}{100} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d99428",
   "metadata": {},
   "source": [
    "### 1.2. Compute the **95% confidence interval** for $\\hat{p}$ using the normal approximation together with the Fisher information matrix (following the example given in the 6th lecture).\n",
    "\n",
    "$f(x; p) = p^x(1-p)^{1-x}$\n",
    "\n",
    "Taking the log of $f(x;p)$\n",
    "\n",
    "$\\log f(x; p) = x\\log p + (1-x)\\log(1-p)$\n",
    "\n",
    "Taking the derivative:\n",
    "\n",
    "$\\frac{\\partial \\log f(x;p)}{\\partial p} = \\frac{x}{p} - \\frac{1-x}{1-p}  $\n",
    "\n",
    "$\\frac{\\partial^2 \\log f(x;p)}{\\partial p^2} = -\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2}  $\n",
    "\n",
    "Using the Fisher information Matrix:\n",
    "\n",
    "$\\mathcal{I}(p) = \\mathbb{E}_p(-\\frac{\\partial^2 \\log f(x;p)}{\\partial p^2}) $\n",
    "\n",
    "Insert formula solved above:\n",
    "\n",
    "$\\mathcal{I}(p) = -\\mathbb{E}_p[-\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2}] $\n",
    "\n",
    "Use same insert logic as in lecture 6 material p.23:\n",
    "\n",
    "$\\mathcal{I}(p) = \\frac{p}{p^2} + \\frac{1-p}{(1-p)^2} $\n",
    "\n",
    "$\\mathcal{I}(p) = \\frac{1}{p} + \\frac{1}{1-p} $\n",
    "\n",
    "$\\mathcal{I}(p) = \\frac{1-p}{p(1-p)} + \\frac{p}{p(1-p)} $\n",
    "\n",
    "$\\mathcal{I}(p) = \\frac{1}{p(1-p)}$\n",
    "\n",
    "$\\hat{se} = \\frac{1}{\\sqrt{\\mathcal I_n (\\hat p_n) }} $\n",
    "\n",
    "$\\hat{se} = \\frac{1}{\\sqrt{n\\mathcal I (\\hat p_n) }} $\n",
    "\n",
    "$\\hat{se} = \\frac{1}{\\sqrt{n \\frac{1}{\\hat p(1-\\hat p)} }} $\n",
    "\n",
    "$\\hat{se} = \\sqrt{\\frac{\\hat p(1- \\hat p)}{n}} $\n",
    "\n",
    "\n",
    "An approximation of 95% confidence interval is:\n",
    "\n",
    "$\\hat p_n \\pm z_{1−\\alpha/2​}\\sqrt{\\frac{\\hat p(1- \\hat p)}{n}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d50f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval: [0.1475, 0.3125]\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "\n",
    "n = 100\n",
    "x = 23\n",
    "p_n = x / n\n",
    "z = norm.ppf(0.975)\n",
    "se = sqrt(p_n * (1 - p_n) / n)\n",
    "lower = p_n - z * se\n",
    "upper = p_n + z * se\n",
    "\n",
    "print(f\"95% confidence interval: [{lower:.4f}, {upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c060b",
   "metadata": {},
   "source": [
    "###   1.3. Compute the same **95% confidence interval** using **percentile** and **pivotal bootstrap**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c159588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Percentile bootstrap CI: [0.1500, 0.3100]\n",
      "95% Pivotal bootstrap CI:    [0.1500, 0.3100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "x = 23\n",
    "sample = np.array([1]*x + [0]*(n - x))\n",
    "\n",
    "p_hat = x / n\n",
    "\n",
    "B = 10000\n",
    "boot_means = []\n",
    "\n",
    "for _ in range(B):\n",
    "    new_sample = np.random.choice(sample, size=n, replace=True)\n",
    "    boot_means.append(np.mean(new_sample))\n",
    "\n",
    "boot_means = np.array(boot_means)\n",
    "\n",
    "ci_percentile = np.percentile(boot_means, [2.5, 97.5])\n",
    "\n",
    "errors = boot_means - p_hat\n",
    "ci_pivotal = [p_hat - np.percentile(errors, 97.5),\n",
    "              p_hat - np.percentile(errors, 2.5)]\n",
    "\n",
    "print(f\"95% Percentile bootstrap CI: [{ci_percentile[0]:.4f}, {ci_percentile[1]:.4f}]\")\n",
    "print(f\"95% Pivotal bootstrap CI:    [{ci_pivotal[0]:.4f}, {ci_pivotal[1]:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a6814",
   "metadata": {},
   "source": [
    "## Problem 2.\n",
    "\n",
    "Let $ X_1, X_2, X_3, X_4 \\sim \\text{Uniform}(a, b) $, where $a$ and $b$ are unknown parameters and $a < b$.\n",
    "\n",
    "### 2.1. Find the MLE $ \\hat{a} $ and $ \\hat{b} $.\n",
    "\n",
    "The PDF is:\n",
    "\n",
    "$$\n",
    "f(x; a, b) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{b - a} & \\text{if } a \\le x \\le b \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "If $\\hat b < X_i$ for some $i\\in {1,2,3,4}$, $f(X_i,\\hat a ,\\hat b) = 0$ \n",
    "\n",
    "and hence $\\mathcal{L_n}(\\hat b) = 0$ for $\\hat b < X_{(n)} = \\max \\{X_1, \\dots, X_4\\}$\n",
    "\n",
    "Similarly, \n",
    "\n",
    "If $\\hat a > X_i$ for some $i\\in {1,2,3,4}$, $f(X_i,\\hat a, \\hat b) = 0$ \n",
    "\n",
    "and hence $\\mathcal{L_n}(\\hat a) = 0$ for $\\hat a > X_{(1)} = \\min \\{X_1, \\dots, X_4\\}$.\n",
    "\n",
    "Solve $\\hat b$ first. For $\\hat b \\geq X{(n)}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_n(b) = \\prod_{i=1}^n f(x_i; a,  b) = (\\frac{1}{b-a})^n\n",
    "$$\n",
    "\n",
    "Since this is strictly decreasing, $\\hat b = X_{(n)} $\n",
    "\n",
    "Solving $\\hat a$ secondly. For $\\hat a \\leq X_{(1)}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_n(a) = \\prod_{i=1}^n f(x_i; a,  b) = (\\frac{1}{b-a})^n\n",
    "$$\n",
    "\n",
    "For maximum likelihood value, $\\hat a$ should be as large as possible, since $(\\frac{1}{b-a})^n$ value grows as $a$ approaches $b$. \n",
    "\n",
    "But if $\\hat a > X_{(1)}$, then $f(X_i,\\hat a, \\hat b) = 0$. Therefore the maximum likelihood value is when $\\hat a = X_{(1)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f1855",
   "metadata": {},
   "source": [
    "### 2.2. \n",
    "\n",
    "Let $ \\tau = \\int x f(x) \\, dx $, where $ f(x)$ is the PDF of the $Uniform(a, b)$ distribution. Find the MLE of $ \\tau $.\n",
    "\n",
    "The PDF is:\n",
    "\n",
    "$$\n",
    "f(x; a, b) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{b - a} & \\text{if } a \\le x \\le b \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$ \\tau = \\int x f(x) \\, dx $\n",
    "\n",
    "$ \\tau = \\int_a^b  x \\cdot \\frac{1}{b-a} \\, dx $\n",
    "\n",
    "$ \\tau = \\frac{1}{b-a} \\int_a^b  x \\, dx $\n",
    "\n",
    "$ \\tau = \\frac{1}{b-a} \\cdot (\\frac{b^2}{2} - \\frac{a^2}{2}) $\n",
    "\n",
    "$ \\tau = \\frac{1}{b-a}\\frac{b^2-a^2}{2} $\n",
    "\n",
    "$ \\tau = \\frac{b^2-a^2}{2(b-a)} $\n",
    "\n",
    "$ \\tau = \\frac{(b+a)(b-a)}{2(b-a)} $\n",
    "\n",
    "$ \\tau = \\frac{b+a}{2} $\n",
    "\n",
    "So maximum likelihood value for $\\hat \\tau$ is:\n",
    "\n",
    "$$ \n",
    "\n",
    "\\hat \\tau = \\frac{\\hat b + \\hat a}{2} \\\\\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat \\tau = \\frac{X_{(n)} + X_{(1)} }{2} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c6ee3",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Let $X_1, ..., X_n \\sim Poisson(\\lambda)$. Find the method of moments estimator for $\\lambda$\n",
    "\n",
    "Following solving logic from lecture 5 material page 11:\n",
    "\n",
    "if $X \\sim Poisson(\\lambda) $ then $\\alpha_1 = E[X] = \\lambda$\n",
    "\n",
    "and\n",
    "\n",
    "$\\hat \\alpha_1 = \\frac{1}{n} \\sum_{i=1}^n X_i$\n",
    "\n",
    "By equating these we get the estimator\n",
    "\n",
    "$\\hat \\lambda_n = \\frac{1}{n} \\sum_{i=1}^n X_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09d564",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Let $X_1, ..., X_n \\sim Poisson(\\lambda)$.Find the the maximum likelihood estimator for $\\lambda$\n",
    "\n",
    "Likelihood function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_n(\\lambda) = \\prod_{i=1}^n f(X_i; \\lambda)\n",
    "$$\n",
    "\n",
    "And the log function:\n",
    "\n",
    "$$\n",
    "\\mathcal{l}_n(\\lambda) = \\log \\mathcal{L}_n(\\lambda) \n",
    "$$\n",
    "\n",
    "Poisson PMF:\n",
    "\n",
    "$$\n",
    "f(k, \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n",
    "$$\n",
    "\n",
    "So likelihood is:          \n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}_n(\\lambda) &= \\prod_{i=1}^n  \\frac{\\lambda^{X_i} e^{-\\lambda}}{X_i!}\\\\\n",
    "\\mathcal{l}_n(\\lambda) &= \\log \\mathcal{L}_n(\\lambda) \\\\\n",
    "\\mathcal{l}_n(\\lambda) &= \\sum_{i=1}^n \\log( \\frac{\\lambda^{X_i} e^{-\\lambda}}{X_i!})\\\\\n",
    "\\mathcal{l}_n(\\lambda) &= \\sum_{i=1}^n \\log(\\lambda^{X_i}) + \\log(e^{-\\lambda}) -\\log(X_i!)\\\\\n",
    "\\mathcal{l}_n(\\lambda) &= \\sum_{i=1}^n \\log(\\lambda)X_i -\\lambda -\\log(X_i!)\\\\\n",
    "\\mathcal{l}_n(\\lambda) &= -n\\lambda - n\\log(X_i!) + \\sum_{i=1}^n (\\log(\\lambda)X_i )\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Taking the derivative\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{l}_n(\\lambda)}{\\partial \\lambda} &= -n + \\sum_{i=1}^n \\frac{d}{d\\lambda}\\log(\\lambda)X_i\\\\\n",
    "\\frac{\\partial \\mathcal{l}_n(\\lambda)}{\\partial \\lambda} &= -n + \\sum_{i=1}^n \\frac{X_i}{\\lambda}\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Solving the equation for 0\n",
    "\n",
    "\\begin{aligned}\n",
    "-n + \\sum_{i=1}^n \\frac{X_i}{\\lambda} &= 0\\\\\n",
    "\\sum_{i=1}^n X_i &= n \\lambda\\\\\n",
    "\\lambda &= \\frac{1}{n} \\sum_{i=1}^n X_i \n",
    "\\end{aligned}\n",
    "\n",
    "Which means MLE $\\hat \\lambda = \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar X = \\lambda$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b724a57",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "week3_exercise5.txt data set comprises samples from the inverse-Gamma distribution which has the pdf\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{b^a}{\\Gamma(a)} x^{-a-1} e^{-\\frac{b}{x}},\n",
    "$$\n",
    "\n",
    "where $0 < a, b$, and $\\Gamma(a)$ denotes the gamma-function.\n",
    "\n",
    "Find the MLE for the parameters $a$ and $b$ using numerical optimization tools.\n",
    "\n",
    "Solving the Likelihood log function:\n",
    "\n",
    "Likelihood function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_n(a, b) = \\prod_{i=1}^n f(X_i, a, b)\n",
    "$$\n",
    "\n",
    "And the log function:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathcal{l}_n(a, b) &= \\log \\mathcal{L}_n(a, b) \\\\\n",
    "\\mathcal{l}_n(a, b) &=  \\log (\\prod_{i=1}^n f(X_i, a, b))\\\\\n",
    "\\mathcal{l}_n(a, b) &= \\sum_{i=1}^n  \\log  f(X_i, a, b)\\\\\n",
    "\\mathcal{l}_n(a, b) &= \\sum_{i=1}^n \\log \\left( \\frac{b^a}{\\Gamma(a)} X_i^{-a-1} e^{-\\frac{b}{X_i}}\\right)\\\\\n",
    "\\mathcal{l}_n(a, b) &= \\sum_{i=1}^n \\left( \\log  \\frac{b^a}{\\Gamma(a)} X_i^{-a-1} + \\log(e^{-\\frac{b}{X_i}})\\right)\\\\\n",
    "\\mathcal{l}_n(a, b) &= \\sum_{i=1}^n \\left( \\log  (b^a X_i^{-a-1}) - \\log(\\Gamma(a)) + \\log(e^{-\\frac{b}{X_i}})\\right)\\\\\n",
    "\\mathcal{l}_n(a, b) &= \\sum_{i=1}^n \\left( \\log  (b^a) + \\log (X_i^{-a-1}) - \\log(\\Gamma(a)) + \\log(e^{-\\frac{b}{X_i}})\\right)\\\\\n",
    "\\mathcal{l}_n(a, b) &= \\sum_{i=1}^n \\left( \\log  (b^a) + \\log (X_i^{-a-1}) - \\log(\\Gamma(a)) - \\frac{b}{X_i}\\right)\\\\\n",
    "\\mathcal{l}_n(a, b) &= \\sum_{i=1}^n \\left( a \\log  (b) + \\log X_i (-a-1) - \\log(\\Gamma(a)) - \\frac{b}{X_i}\\right)\\\\\n",
    "\\mathcal{l}_n(a, b) &= n a \\log (b) - n \\log(\\Gamma(a))  + \\sum_{i=1}^n \\left(\\log X_i (-a-1) - \\frac{b}{X_i}\\right)\\\\\n",
    "- \\mathcal{l}_n(a, b) &= -\\left( n a \\log (b) - n \\log(\\Gamma(a))  + \\sum_{i=1}^n \\left(\\log X_i (-a-1) - \\frac{b}{X_i}\\right)\\right)\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d94010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n",
      "Estimated a and b: 1.0048346829701857 2.0152455483516474\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln \n",
    "from scipy.optimize import minimize\n",
    "def get_data():\n",
    "\n",
    "    array =[]\n",
    "    with open(\"week3_exercise5.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"X\" in line:\n",
    "                continue\n",
    "            array.append(float(line.strip()))\n",
    "    return np.array(array)\n",
    "\n",
    "def inverse_gamma_function(params, data):\n",
    "    a, b = params\n",
    "    if a <= 0 or b <= 0:\n",
    "        return np.inf\n",
    "    n = len(data)\n",
    "    return -(n * a * np.log(b) - n*gammaln(a) + np.sum(np.log(data))*(-a-1) - np.sum(b/data))\n",
    "\n",
    "data = get_data()\n",
    "\n",
    "result = minimize(\n",
    "    inverse_gamma_function,\n",
    "    [0.5, 10],\n",
    "    args=(data,),\n",
    "    method=\"L-BFGS-B\",\n",
    "    bounds=[(0, None)]\n",
    ")\n",
    "\n",
    "print(\"Success:\", result.success)\n",
    "print(\"Estimated a and b:\", result.x[0], result.x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
